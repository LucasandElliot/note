# 大数据技术笔记

## MapReduce

### MapReduce模型简介

一般是用于mster-slave框架，master运行jobTracer， slave运行TaskTracer

- Map函数

  Map函数应用于将数据解析为<key,value>，输入为<k1,v1>,输出为list(<k1,v2>)

- Reduce函数

  输入为list(<k1,v2>)，输出为同一个批次的<k3, v3>

执行阶段为以下几个步骤。

1. 输入

2. 切片

   HDFS是按照固定block为基本单位存储，因此，处理单位为split（包含元数据，数据长度，节点位置等，起始信息）用户自定义，与HDFS的block无一定关系

  -  ![image-20231117102948719](https://cdn.jsdelivr.net/gh/LucasandElliot/note/big_data/src/202311171029755.png)

3. Map

   理想为一个split等于一个block

4. Shuffle

   Map之后进入缓存，随后进行溢写（分区，排序，合并），磁盘归并，随后被Reduce取走

   归并（combine）和合并（merge）的区别：两个键值对<“a”,1>和<“a”,1>，如果合并，会得到<“a”,2>，如果归并，会得到<“a”,<1,1>>

   （合并为加法，归并为整合，但不想加）

    - ![image-20231117103540886](https://cdn.jsdelivr.net/gh/LucasandElliot/note/big_data/src/202311171035922.png)

    - ![image-20231117103549800](https://cdn.jsdelivr.net/gh/LucasandElliot/note/big_data/src/202311171035837.png)

5. Reduce

 - ![image-20231117102802361](https://cdn.jsdelivr.net/gh/LucasandElliot/note/big_data/src/202311171028404.png)

### MapReduce应用

- 关系代数运算（选择，投影，交，并，差，连接）
- 分组聚合
- 矩阵-向量乘法
- 矩阵乘法

### MapReduce代码编写（以词频统计为例）

- MapReduce输入

  期望为<key,value>，在java中<Object,Text>

- MapReduce输出

  期望为<单词，出现次数>，在java中为<Text,IntWritable>

- Reduce输入

  期望为<key,Iterable容器>

在java中，都需要extend发方法为Map或Reduce，在main方法里面需要设置如下。

```
public static void main(String[] args) throws Exception{  
	    Configuration conf = new Configuration();  
	    String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs();  
	                if (otherArgs.length != 2)  
	                {  
	                        System.err.println("Usage: wordcount <in> <out>");  
	                        System.exit(2);  
	                }  
	                Job job = new Job(conf,"word count");  
	                job.setJarByClass(WordCount.class);  
	                job.setMapperClass(MyMapper.class);  
	                job.setReducerClass(MyReducer.class);  
	                job.setOutputKeyClass(Text.class);  
	                job.setOutputValueClass(IntWritable.class);  
	                FileInputFormat.addInputPath(job,new Path(otherArgs[0]));  
	                FileOutputFormat.setOutputPath(job,new Path(otherArgs[1]));  
	                System.exit(job.waitForCompletion(true)?0:1);  
	        }  

```

### MapReduce命令行编译打包代码以及运行jar可执行文件

#### 命令行编译

添加环境变量到~/.bashrc中（环境变量）

```
export HADOOP_HOME=/usr/local/hadoop

export CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATH
```

- $HADOOP_HOME/share/hadoop/common/hadoop-common-3.1.3.jar

- $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.3.jar

- $HADOOP_HOME/share/hadoop/common/lib/commons-cli-1.2.jar

编译代码(前提是需要将hadoop的classpath加入到环境变量中)

```
javac wordcount.java
jar -cvf wordcount.jar ./wordcount*.class
```

#### IDE执行编译程序

必须需要的jar包如下（运用eclipse或者idea导包）

- “/usr/local/hadoop/share/hadoop/common”目录下的hadoop-common-3.1.3.jar和haoop-nfs-3.1.3.jar；

- “/usr/local/hadoop/share/hadoop/common/lib”目录下的所有JAR包

- “/usr/local/hadoop/share/hadoop/mapreduce”目录下的所有JAR包

- “/usr/local/hadoop/share/hadoop/mapreduce/lib”目录下的所有JAR包

具体命令如下所示。

```
# 删除hadoop所在的文件夹
cd /usr/local/hadoop
./bin/hdfs dfs -rm -r output
./bin/hdfs dfs -rm -r input
# 创建hadoop文件夹
cd /usr/local/hadoop
./bin/hdfs dfs -mkdir input
# 将数据放置在hadoop制定文件夹中
cd /usr/local/hadoop
./bin/hdfs dfs -put ./wordfile1.txt input
./bin/hdfs dfs -put ./wordfile2.txt input
# 删除hadoop所在的文件夹
cd /usr/local/hadoop
./bin/hdfs dfs -rm -r /user/hadoop/output
# 运行已经打包好的java包，参数为jar 输入文件夹 输出文件夹
cd /usr/local/hadoop
./bin/hadoop jar ./myapp/WordCount.jar input output
# 输出结果
cd /usr/local/hadoop
./bin/hdfs dfs -cat output/*



```

## Hive

### Hive简介

构建于Hadoop顶层的数据仓库工具。本身不存储和处理数据，依赖于HDFS存储数据，依赖分布式并行文件系统MapReduce处理数据，主要是定义了HiveSQL（类似于SQL）查询语言，通过编写HiveSQL运行MapReduce任务，具有较多数据类型

#### 特点

- 支持批处理方式处理海量数据
- 支持对数据加载转化，可以查询，存储，分析大规模数据
- 处理静态数据，能够与Hbase互补（hbase能够提供实时访问功能）
- 不支持数据更新，支持分区以及索引
- 图示
  - ![image-20231117114310070](https://cdn.jsdelivr.net/gh/LucasandElliot/note/big_data/src/202311171143104.png)

#### 优势

- 在数据插入志宏，支持批量传入数据，为全表扫描
- 不支持数据更新，Hive存放静态数据
- 提供有限的索引

## Hive系统架构

- 用户接口模块

  包括CLI, HWJ, JDBC, ODBC, Thirft Sever，CLI为自带命令行界面， HWI为简单网页界面，JDBC，ODBC和Thrift Server是向用户提供编程访问的接口

- 驱动模块

  包括编译器，优化器，执行器，所有指令都会进入到驱动模块，按照步骤执行

  编译器为Antlr语言识别工具，将SQL转化为抽象语法树（AST tre），遍历为QueryBlock查询单元（包含输入源，计算过程，输出部分）

- 元数据存储模块

  独立关系型数据库，也可以为MYSQL连接，也可以为自带Derby数据库，保存表名称，表列以及属性，表的分区以及属性，表的数据所在位置信息

- 图示

  - ![image-20231117115109734](https://cdn.jsdelivr.net/gh/LucasandElliot/note/big_data/src/202311171151767.png)

### 查询执行过程

1. 用CLI输入命令
2. 编译器Antlr语言识别工具对SQL语言此法和语法解析，转化为抽象语法树（AST TREE）
3. 对AST TREE 遍历，转化为QueryBlock查询单元（基本SQL语法组成单元，包括输入源，计算过程和输出）
4. 对QueryBlock进行遍历，生成执行操作树（OperatorTree）
