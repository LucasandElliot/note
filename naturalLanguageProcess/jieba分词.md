<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->


- [jieba分词简介](#jieba%E5%88%86%E8%AF%8D%E7%AE%80%E4%BB%8B)
- [jieba分词原理说明](#jieba%E5%88%86%E8%AF%8D%E5%8E%9F%E7%90%86%E8%AF%B4%E6%98%8E)
  - [例子](#%E4%BE%8B%E5%AD%90)
- [参考](#%E5%8F%82%E8%80%83)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# jieba分词简介

jieba分词为一个中文文本处理的开源分词工具，是一个基于统计和词典的分词工具，具有简单易用，可拓展性强等特点，被广泛应用于中文自然语言处理任务中如句子分词、信息检索等领域。jieba分词主要支持三种分词形式，分别为精确模式将句子精确分开，一般用于文本分析；全模式将句子所有可能成词构建成一个集合，速度较快，不适用于文本分析；搜索引擎模式，在精确模式的前提下，对场次再次切分，获取细粒度更高的分词结果，提高召回率。此外，jieba分词支持自定义词典，可以针对不同领域，获取多层次分词结果。jieba分词运用了基于前缀词典的动态规划分词算法和并行分词，具有较高的分词速度，适用于大规模文本处理。

  jieba分词的具体流程为初始化词典文件，切分短语，将文本切分为多个句子集合，随后构建所有可能分词结果有向无环图（DAG），构建节点最大路径概率以及结束位置，随后构建切分组合，根据节点路径，得到词语切分结果。若词语在词典中，直接返回，若词语不在词典中，运用隐马尔可夫模型（HMM）处理未登陆词并返回结果。

# jieba分词原理说明

主要是通过词典分词，若词语不在词典之中，使用HMM算法识别新词。具体流程为如下所示。

1. 初始化，加载词典文件，获取词语以及出现的次数
2. 切分短语，利用正则表达式，将文本切分为多个语句（句子），之后对语句进行分词
3. 构建DAG，通过字符串匹配，构建所有可能分词结果的有向无环图（DAG）
4. 构建节点最大路径概率以及结束位置，计算每个汉字节点到句尾结尾中所有路径中最大概率，并记录在最大概率的时候DAG中对应汉字成词的结束文字
5. 构建切分组合，根据节点路径，得到词语切分结果
6. 如果该词语不在词典中，运用统计方法HMM（隐马尔可夫模型）处理
7. 运用yield语句返回逐个返回分词结果

## 例子

运用有向无环图的查找算法，通过动态规划，使得词语的切割组合联合概率最大，下面以“春雨医生你好全身红疹子”为例子。

假设词语词典如下

| 词语             | 次数  | 词性 |
| :--------------: | ---- | :-------: |
| 医生 | 100  | n         |
|春雨|100|n|
| 春雨医生 | 100  | n    |
| 你好 | 100 | n |
| 全身 | 100  | n    |
| 红疹 | 100 | n |
| 疹子 | 100 | n |

无环有向图如下所示

![image-20231213111634677](src/image-20231213111634677.png)



# 参考

[jieba分词介绍](https://www.cnblogs.com/foley/p/12789634.html)
